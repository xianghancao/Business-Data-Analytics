{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af9ce21-c654-425c-9db9-8c7725cdf020",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 线性回归的理论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df15a68-9f8d-421c-ad28-fc11b65180f5",
   "metadata": {},
   "source": [
    "## 1. 相关系数的理论\n",
    "我们先考虑下两个连续变量之间的统计关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ab8ed-2310-4954-87fe-cf359b7dd655",
   "metadata": {},
   "source": [
    "![simple_regression](image/simple_regression.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f84533-0b76-433a-be82-df8908398b12",
   "metadata": {
    "tags": []
   },
   "source": [
    "在线性回归分析开始前，一般计算自变量和因变量的相关系数，我们把这一步称之为相关分析。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726502c-d3c6-4521-abba-416ac84d7f0f",
   "metadata": {},
   "source": [
    "相关系数r检验y和x两个变量之间的线性相关的显著程度，其算式为\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum{(x_i-\\bar{x})^2\\sum{(y_i-\\bar{y})^2}}}}\n",
    "$$\n",
    "\n",
    "数学上可以证明：r在[-1, 1]范围，有：\n",
    "- r>0时，y与x有一定的正线性相关，越接近1正的相关性越大\n",
    "- r<0时，y与x有一定的负线性相关，越接近-1负的相关性越大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b90177-a6e2-4a4d-9246-71b2406dd6b0",
   "metadata": {},
   "source": [
    "## 2. 一元线性回归的理论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b78fb9-3ce7-4f5b-81d3-e9d70a44feaa",
   "metadata": {},
   "source": [
    "**输出变量Y**被称为被解释变量、因变量、结果，而**输入变量X**可以被称为解释变量、自变量、效果、预测因子。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba9a0d-f677-4ca5-b66b-1b54337c4359",
   "metadata": {},
   "source": [
    "$$\n",
    "Y_i = \\beta_0+\\beta_1X_i+u_i \n",
    "$$\n",
    "\n",
    "$$\n",
    "i是第i次观测，i=1,2,...,n;Y_i是被解释变量，\\beta_0是截距；\\beta_1是总体回归线的斜率，u_i是误差项\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559987f1-1adc-40cd-8d9e-cca5ac14f3c5",
   "metadata": {},
   "source": [
    "线性回归拟合一个具有系数的线性模型，以最小化数据集内观测目标与线性逼近预测目标之间的残差平方和。数学上，它解决了这样一个问题:\n",
    "\n",
    "$$\n",
    "min\\{\\sum^{n}_{i=1}(Y-\\beta_0-\\beta_1X_i)^2\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee6c5c-c51e-4d83-b626-913d62a64be6",
   "metadata": {},
   "source": [
    "### OLS方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8150670-86ae-4490-a818-67047551a222",
   "metadata": {},
   "source": [
    "![OLS](image/OLS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38128704-5ad7-446f-ae5e-48834e3431b8",
   "metadata": {},
   "source": [
    "为了最小化预测误差平方和$\\sum^{n}_{i=1}(Y-\\beta_0-\\beta_1X_i)^2$，首先将该式关于$b_0$和$b_1$求偏导数，可以得到以下两个等式：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sum(Y_i-\\beta_0-\\beta_1X_i)^2}{\\partial\\beta_0}\n",
    "= -2\\sum(Y_i-\\beta_0-\\beta_1X_i)^2\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\sum(Y_i-\\beta_0-\\beta_1X_i)^2}{\\partial\\beta_1}\n",
    "= -2\\sum(Y_i-\\beta_0-\\beta_1X_i)X_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f9020-5010-422b-a546-083709dcf3c7",
   "metadata": {},
   "source": [
    "令上面2个偏导数等于零，整理后得到OLS估计量$\\beta_0$和$\\beta_1$必须满足的两个方程：\n",
    "\n",
    "$$\n",
    "\\bar{Y}-\\hat{\\beta_0}-\\hat{\\beta_1}\\bar{X}=0\n",
    "$$\n",
    "$$\n",
    "\\frac{1}{n}\\sum{X_i}{Y_i} - \\hat{\\beta_0}\\bar{X}-\\hat{\\beta_1}\\frac{1}{n}\\sum^n_{i=1}X^2_i = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b89db-bb5b-484b-a2c4-7a325a830948",
   "metadata": {},
   "source": [
    "解上述关于$\\hat{\\beta_0}$和$\\hat{\\beta_1}$的方程组，得到\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_1}=\\frac{\\frac{1}{n}\\sum_{i=1}^n X_i Y_i-\\bar{XY}}{\\frac{1}{n}\\sum_{i=1}^n X_i^2 - \\bar{X}^2}\n",
    "$$\n",
    "$$\n",
    "\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88880618-3dec-4f68-bfa6-f5285500ae11",
   "metadata": {},
   "source": [
    "## 3. 多元线性回归的理论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ccaac-6557-4291-a7cd-35b768a0bb8a",
   "metadata": {},
   "source": [
    "![multi_regression](image/multi_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb9edc-50c8-4b2c-8a0a-a837dc670431",
   "metadata": {},
   "source": [
    "多元回归模型是：\n",
    "\n",
    "$$\n",
    "Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ... +\\beta_kX_{ki} + \\mu_i,i=1,...,n\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $Y_i$是被解释变量的第$i$个观测值；$X_{1i},X_{2i},...,X_{ki}$是$k$个解释变量的第$i$个观测值；$\\mu_i$是误差项。\n",
    "- 总体回归线表示的是$Y$和$X$之间的总体平均关系。\n",
    "- $\\beta_1$是$X_1$的斜率系数；$\\beta2$是$X_2$的斜率系数，等等。\n",
    "- 截距$\\beta_0$是当所有解释变量$X$取值为零时$Y$期望值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56364176-1fb2-4852-9597-2ccb4179e357",
   "metadata": {},
   "source": [
    "\n",
    "估计量$\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_k}$为使得预测误差平方和$\\sum^n_{i=1}(Y_i-\\beta_0-\\beta_1X_{1i}-...-\\beta_kX_{ki})^2$达到最小的$\\beta_0,\\beta_1,...,\\beta_k$取值。\n",
    "\n",
    "预测值$\\hat{Y_i}$和残差$\\hat{u_i}$分别为：\n",
    "\n",
    "$$\n",
    "\\hat{Y_i}=\\hat{\\beta_0} + \\hat{\\beta_1}X_{1i} + ... + \\hat{\\beta_k}X_{ki}, i=1,...,n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{u_i}=Y_i-\\hat{Y_i}, i=1,...,n\n",
    "$$\n",
    "\n",
    "其中估计量$\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_k}$和残差$\\hat{u_i}$都是利用$n$组样本观测数据$(X_{1i}, ..., X_{ki},Y_i), i=1,...n$计算得到的。它们分别是未知真实总体系数$\\beta_0,\\beta_1,...,\\beta_k$和误差项$\\mu_i$的估计量。\n",
    "\n",
    "我们使用OLS方法求的$\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_k}$，为普通最小二乘(OLS)估计量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f1a6e-6790-4933-afc0-961890034653",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 模型评价指标\n",
    "\n",
    "### SER\n",
    "\n",
    "回归标准误（SER）是对误差项$\\mu_i$标准差的估计，故SER可用来度量$Y$的分布在回归线周围的离散程度。\n",
    "\n",
    "$$\n",
    "SER = s_{\\hat{u}}=\\sqrt{s^2_{\\hat{u}}}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866285f-983f-460f-a035-920b5deadac4",
   "metadata": {},
   "source": [
    "\n",
    "### $R^2$\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{ESS}{TSS}=\\frac{\\sum^n_{i=1}{(\\hat{Y_i}-\\bar{Y})^2}}{\\sum^n_{i=1}{(Y_i-\\bar{Y})^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{ESS}{TSS}=1-\\frac{SSR}{TSS}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R^2 = 1-\\frac{SSR}{TSS}\n",
    "$$\n",
    "\n",
    "$$\n",
    "SSR为残差平方和\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5474d-daa7-48b7-85f6-d5d73709f2df",
   "metadata": {},
   "source": [
    "\n",
    "### 调整$R^2$\n",
    "\n",
    "$$\n",
    "\\bar{R}^2=1-\\frac{n-1}{n-k-1}\\frac{SSR}{TSS}=1-\\frac{s^2_\\hat{u}}{s^2_Y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "k:斜率系数和截距的数量\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a2a99-24ef-4a0e-a87a-9ec78689302e",
   "metadata": {},
   "source": [
    "## 参考\n",
    "1. https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-regression.html\n",
    "2. 詹姆斯\\*斯托克，马克\\*沃森《计量经济学》第三版"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
