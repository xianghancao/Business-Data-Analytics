{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af9ce21-c654-425c-9db9-8c7725cdf020",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 线性回归的理论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df15a68-9f8d-421c-ad28-fc11b65180f5",
   "metadata": {},
   "source": [
    "## 1. 相关系数的理论\n",
    "我们先考虑下两个连续变量之间的统计关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ab8ed-2310-4954-87fe-cf359b7dd655",
   "metadata": {},
   "source": [
    "![simple_regression](image/simple_regression.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f84533-0b76-433a-be82-df8908398b12",
   "metadata": {
    "tags": []
   },
   "source": [
    "在线性回归分析开始前，一般计算自变量和因变量的相关系数，我们把这一步称之为相关分析。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726502c-d3c6-4521-abba-416ac84d7f0f",
   "metadata": {},
   "source": [
    "相关系数r检验y和x两个变量之间的线性相关的显著程度，其算式为\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum{(x_i-\\bar{x})^2\\sum{(y_i-\\bar{y})^2}}}}\n",
    "$$\n",
    "\n",
    "数学上可以证明：r在[-1, 1]范围，有：\n",
    "- r>0时，y与x有一定的正线性相关，越接近1正的相关性越大\n",
    "- r<0时，y与x有一定的负线性相关，越接近-1负的相关性越大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b90177-a6e2-4a4d-9246-71b2406dd6b0",
   "metadata": {},
   "source": [
    "## 2. 一元线性回归的理论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c760c-d677-442d-9f65-03e4eaa2e577",
   "metadata": {},
   "source": [
    "一元线性回归是统计学中的一种线性回归模型，用于建立一个因变量（也称为响应变量、被解释变量）与一个自变量（也称为解释变量）之间的线性关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8769542-3284-476f-b7f9-27cfaa61ccbb",
   "metadata": {},
   "source": [
    "### 2.1 解释变量和被解释变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54884d27-ea22-4d6e-92b1-36b5849d5ad5",
   "metadata": {},
   "source": [
    "一元线性回归的表达形式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78be99e-4c3d-4d9d-a720-be839984fa9c",
   "metadata": {},
   "source": [
    "$$\n",
    "Y_i = \\beta_0+\\beta_1X_i+u_i \n",
    "$$\n",
    "\n",
    "$$\n",
    "i是第i次观测，i=1,2,...,n;Y_i是被解释变量，\\beta_0是截距；\\beta_1是总体回归线的斜率，u_i是误差项\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6633e-ce7e-40ca-9239-896a863a6f49",
   "metadata": {},
   "source": [
    "输出变量 $Y$ 被称为被解释变量、因变量、响应变量、结果，而输入变量 $X$ 可以被称为解释变量、自变量、预测因子。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5bd299-0f91-4c75-860e-b8d71c3785b3",
   "metadata": {},
   "source": [
    "## 2.2 最小二乘法（OLS方法）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e707a1-c620-44da-9ad2-9fd43a7fa5ee",
   "metadata": {},
   "source": [
    "<img src='image/OLS.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b1e66-aa5f-4f22-911a-daeaf05371d7",
   "metadata": {},
   "source": [
    "线性回归拟合一个具有系数的线性模型，以最小化数据集内观测目标与线性逼近预测目标之间的残差平方和。数学上，它解决了这样一个问题:\n",
    "\n",
    "$$\n",
    "min\\{\\sum^{n}_{i=1}(Y-\\hat{Y_i})^2\\}\n",
    "$$\n",
    "\n",
    "这里的 $Y_i$ 为观测的值，$\\hat{Y_i}$ 为预测值。\n",
    "\n",
    "因为 $\\hat{Y_i}$ 满足直线方程：$\\hat{Y_i} = \\beta_0+\\beta_1X_i$ ，代入上式后，目标函数变成：\n",
    "\n",
    "\n",
    "$$\n",
    "min\\{\\sum^{n}_{i=1}(Y-\\beta_0-\\beta_1X_i)^2\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1014a140-81b1-49c0-9e4f-c7cb5886231e",
   "metadata": {},
   "source": [
    "为了最小化预测误差平方和$\\sum^{n}_{i=1}(Y-\\beta_0-\\beta_1X_i)^2$，首先将该式关于$\\beta_0$和$\\beta_1$求偏导数，可以得到以下两个等式：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sum(Y_i-\\beta_0-\\beta_1X_i)^2}{\\partial\\beta_0}\n",
    "= -2\\sum(Y_i-\\beta_0-\\beta_1X_i)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\sum(Y_i-\\beta_0-\\beta_1X_i)^2}{\\partial\\beta_1}\n",
    "= -2\\sum(Y_i-\\beta_0-\\beta_1X_i)X_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919467a7-2858-400e-a5b6-6d95fa74fb2c",
   "metadata": {},
   "source": [
    "令上面2个偏导数等于零，整理后得到OLS估计量 $\\hat{\\beta_0}$ 和 $\\hat{\\beta_1}$ 必须满足的两个方程：\n",
    "\n",
    "$$\n",
    "\\bar{Y}-\\hat{\\beta_0}-\\hat{\\beta_1}\\bar{X}=0\n",
    "$$\n",
    "$$\n",
    "\\frac{1}{n}\\sum{X_i}{Y_i} - \\hat{\\beta_0}\\bar{X}-\\hat{\\beta_1}\\frac{1}{n}\\sum^n_{i=1}X^2_i = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b37fe-d487-4bc9-ae1c-7c5d34c93936",
   "metadata": {},
   "source": [
    "这里的 $\\bar{Y}$ 为 $Y_i$ 的均值，$\\bar{X}$ 为 $X_i$ 的均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f656e-564a-462a-a062-fd9553b51f30",
   "metadata": {},
   "source": [
    "解上述关于$\\hat{\\beta_0}$和$\\hat{\\beta_1}$的方程组，得到\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}\n",
    "$$\n",
    "$$\n",
    "\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1 \\bar{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88880618-3dec-4f68-bfa6-f5285500ae11",
   "metadata": {},
   "source": [
    "## 3. 多元线性回归的理论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ccaac-6557-4291-a7cd-35b768a0bb8a",
   "metadata": {},
   "source": [
    "![multi_regression](image/multi_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172e396-5424-465f-88c1-b43a77c4e2da",
   "metadata": {},
   "source": [
    "作为一元线性回归的扩展形式，多元线性回归用于建立一个因变量与多个自变量之间的线性关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb9edc-50c8-4b2c-8a0a-a837dc670431",
   "metadata": {},
   "source": [
    "多元回归模型是：\n",
    "\n",
    "$$\n",
    "Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ... +\\beta_kX_{ki} + \\mu_i,i=1,...,n\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $Y_i$是被解释变量的第$i$个观测值；$X_{1i},X_{2i},...,X_{ki}$是$k$个解释变量的第$i$个观测值；$\\mu_i$是误差项。\n",
    "- 总体回归线表示的是$Y$和$X$之间的总体平均关系。\n",
    "- $\\beta_1$是$X_1$的斜率系数；$\\beta2$是$X_2$的斜率系数，等等。\n",
    "- 截距$\\beta_0$是当所有解释变量$X$取值为零时$Y$期望值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56364176-1fb2-4852-9597-2ccb4179e357",
   "metadata": {},
   "source": [
    "\n",
    "估计量$\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_k}$为使得预测误差平方和$\\sum^n_{i=1}(Y_i-\\beta_0-\\beta_1X_{1i}-...-\\beta_kX_{ki})^2$达到最小的$\\beta_0,\\beta_1,...,\\beta_k$取值。\n",
    "\n",
    "预测值$\\hat{Y_i}$和残差$\\hat{u_i}$分别为：\n",
    "\n",
    "$$\n",
    "\\hat{Y_i}=\\hat{\\beta_0} + \\hat{\\beta_1}X_{1i} + ... + \\hat{\\beta_k}X_{ki}, i=1,...,n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{u_i}=Y_i-\\hat{Y_i}, i=1,...,n\n",
    "$$\n",
    "\n",
    "其中估计量$\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_k}$和残差$\\hat{u_i}$都是利用$n$组样本观测数据$(X_{1i}, ..., X_{ki},Y_i), i=1,...n$计算得到的。它们分别是未知真实总体系数$\\beta_0,\\beta_1,...,\\beta_k$和误差项$\\mu_i$的估计量。\n",
    "\n",
    "我们使用OLS方法求的$\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_k}$，为普通最小二乘(OLS)估计量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f1a6e-6790-4933-afc0-961890034653",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 模型评价指标\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11395737-161b-49dd-8e19-35fda69c87c8",
   "metadata": {},
   "source": [
    "### 4.1 SER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f162a49-a3ce-40c6-9e50-3d28ef5ea35e",
   "metadata": {},
   "source": [
    "回归标准误（SER）是指对回归系数的标准误。在统计学中，标准误是一个用于衡量样本统计量与总体统计量之间差异的指标。对于回归分析中的回归系数，标准误可以帮助我们评估回归系数的稳定性和可靠性。\n",
    "\n",
    "对于简单线性回归模型，回归标准误可以通过下式计算得到：\n",
    "\n",
    "$$ SER = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n-2}} $$\n",
    "\n",
    "其中：\n",
    "- $ y_i $ 表示第 $ i $ 个观测值的因变量值；\n",
    "- $ \\hat{y}_i $ 表示第 $ i $ 个观测值的预测值（根据回归方程预测的值）；\n",
    "- $ n $ 表示样本数量。\n",
    "\n",
    "\n",
    "回归标准误的意义在于：\n",
    "- 它是回归系数估计值的标准差，用于评估回归系数的稳定性和精确度。\n",
    "- 它可以用于构建置信区间和进行假设检验，例如t检验。\n",
    "- 通过比较不同模型的回归标准误，可以帮助我们选择最合适的模型。\n",
    "\n",
    "总之，回归标准误是回归分析中一个重要的指标，用于评估回归系数的可靠性和模型的拟合效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e784cf0b-8c84-4027-93a3-689689ce026e",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 R平方\n",
    "\n",
    "$R^2$（R-squared，R平方）是回归分析中用于评估模型拟合优度的统计指标之一。它是一个介于0和1之间的值，表示因变量的变异性中被自变量解释的比例。$R^2$越接近1，说明模型拟合得越好；$R^2$越接近0，则说明模型对数据的拟合程度较差。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd31ef9d-090a-4abf-9992-7767f06b18f8",
   "metadata": {},
   "source": [
    "\n",
    "$R^2 $ 的计算公式如下：\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{ESS}{TSS}=\\frac{\\sum^n_{i=1}{(\\hat{Y_i}-\\bar{Y})^2}}{\\sum^n_{i=1}{(Y_i-\\bar{Y})^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{ESS}{TSS}=1-\\frac{SSR}{TSS}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R^2 = 1-\\frac{SSR}{TSS}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ SSR $ 是残差平方和，表示模型预测值与实际观测值之间的差异的平方和；\n",
    "- $ TSS $ 是总平方和，表示实际观测值与因变量均值之间的差异的平方和。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04498f1-f984-41ef-b03d-961e1a1820c7",
   "metadata": {},
   "source": [
    "\n",
    "换句话说，$R^2$ 是残差平方和占总平方和的比例的补数。它衡量了模型所能解释的因变量变异性的比例，即模型对数据的拟合程度。$R^2$ 的取值范围为0到1，其中1表示模型完美拟合数据，0表示模型未能解释因变量的任何变异性。\n",
    "\n",
    "在回归分析中，$R^2$ 通常作为一个重要的指标来评估模型的拟合优度。然而，需要注意的是，$R^2$ 并不是模型拟合程度的唯一评价指标，有时候模型的解释能力可能不够好，但 $R^2$ 仍然较高，因此在进行模型评估时，需要结合其他指标进行综合考虑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a2a99-24ef-4a0e-a87a-9ec78689302e",
   "metadata": {},
   "source": [
    "## 参考\n",
    "1. https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-regression.html\n",
    "2. 詹姆斯\\*斯托克，马克\\*沃森《计量经济学》第三版"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
